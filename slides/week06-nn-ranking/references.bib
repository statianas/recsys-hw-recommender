@inproceedings{YTBE,
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
title = {Deep Neural Networks for YouTube Recommendations},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959190},
doi = {10.1145/2959100.2959190},
abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation
systems in existence. In this paper, we describe the system at a high level and focus
on the dramatic performance improvements brought by deep learning. The paper is split
according to the classic two-stage information retrieval dichotomy: first, we detail
a deep candidate generation model and then describe a separate deep ranking model.
We also provide practical lessons and insights derived from designing, iterating and
maintaining a massive recommendation system with enormous user-facing impact.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {191–198},
numpages = {8},
keywords = {deep learning, recommender system, scalability},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}
 
@inproceedings{AIRBNB,
author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
title = {Applying Deep Learning to Airbnb Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330658},
doi = {10.1145/3292500.3330658},
abstract = {The application to search ranking is one of the biggest machine learning success stories
at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree
model. The gains, however, plateaued over time. This paper discusses the work done
in applying neural networks in an attempt to break out of that plateau. We present
our perspective not with the intention of pushing the frontier of new modeling techniques.
Instead, ours is a story of the elements we found useful in applying neural networks
to a real life product. Deep learning was steep learning for us. To other teams embarking
on similar journeys, we hope an account of our struggles and triumphs will provide
some useful pointers. Bon voyage!},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1927–1935},
numpages = {9},
keywords = {deep learning, search ranking, e-commerce},
location = {Anchorage, AK, USA},
series = {KDD '19}
}



@inproceedings{PINSAGE,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219890},
doi = {10.1145/3219819.3219890},
abstract = {Recent advancements in deep neural networks for graph-structured data have led to
state-of-the-art performance on recommender system benchmarks. However, making these
methods practical and scalable to web-scale recommendation tasks with billions of
items and hundreds of millions of users remains an unsolved challenge. Here we describe
a large-scale deep recommendation engine that we developed and deployed at Pinterest.
We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines
efficient random walks and graph convolutions to generate embeddings of nodes (i.e.,
items) that incorporate both graph structure as well as node feature information.
Compared to prior GCN approaches, we develop a novel method based on highly efficient
random walks to structure the convolutions and design a novel training strategy that
relies on harder-and-harder training examples to improve robustness and convergence
of the model. We also develop an efficient MapReduce model inference algorithm to
generate embeddings using a trained model. Overall, we can train on and embed graphs
that are four orders of magnitude larger than typical GCN implementations. We show
how GCN embeddings can be used to make high-quality recommendations in various settings
at Pinterest, which has a massive underlying graph with 3 billion nodes representing
pins and boards, and 17 billion edges. According to offline metrics, user studies,
as well as A/B tests, our approach generates higher-quality recommendations than comparable
deep learning based systems. To our knowledge, this is by far the largest application
of deep graph embeddings to date and paves the way for a new generation of web-scale
recommender systems based on graph convolutional architectures.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {974–983},
numpages = {10},
keywords = {recommender systems, deep learning, scalability, graph convolutional networks},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{RANKING,
author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
title = {Recommending What Video to Watch next: A Multitask Ranking System},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346997},
doi = {10.1145/3298689.3346997},
abstract = {In this paper, we introduce a large scale multi-objective ranking system for recommending
what video to watch next on an industrial video sharing platform. The system faces
many real-world challenges, including the presence of multiple competing ranking objectives,
as well as implicit selection biases in user feedback. To tackle these challenges,
we explored a variety of soft-parameter sharing techniques such as Multi-gate Mixture-of-Experts
so as to efficiently optimize for multiple ranking objectives. Additionally, we mitigated
the selection biases by adopting a Wide and Deep framework. We demonstrated that our
proposed techniques can lead to substantial improvements on recommendation quality
on one of the world's largest video sharing platforms.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {43–51},
numpages = {9},
keywords = {selection bias, recommendation and ranking, multitask learning},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{CB2CF,
author = {Barkan, Oren and Koenigstein, Noam and Yogev, Eylon and Katz, Ori},
title = {CB2CF: A Neural Multiview Content-to-Collaborative Filtering Model for Completely Cold Item Recommendations},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3347038},
doi = {10.1145/3298689.3347038},
abstract = {In Recommender Systems research, algorithms are often characterized as either Collaborative
Filtering (CF) or Content Based (CB). CF algorithms are trained using a dataset of
user preferences while CB algorithms are typically based on item profiles. These approaches
harness different data sources and therefore the resulting recommended items are generally
very different. This paper presents the CB2CF, a deep neural multiview model that
serves as a bridge from items content into their CF representations. CB2CF is a "real-world"
algorithm designed for Microsoft Store services that handle around a billion users
worldwide. CB2CF is demonstrated on movies and apps recommendations, where it is shown
to outperform an alternative CB model on completely cold items.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {228–236},
numpages = {9},
keywords = {multiview representation learning, cold item recommendations},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{DCN,
author = {Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed},
title = {DCN V2: Improved Deep and Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450078},
doi = {10.1145/3442381.3450078},
abstract = {Learning effective feature crosses is the key behind building recommender systems.
However, the sparse and large feature space requires exhaustive search to identify
effective crosses. Deep and Cross Network (DCN) was proposed to automatically and efficiently
learn bounded-degree predictive feature interactions. Unfortunately, in models that
serve web-scale traffic with billions of training examples, DCN showed limited expressiveness
in its cross network at learning more predictive feature interactions. Despite significant
research progress made, many deep learning models in production still rely on traditional
feed-forward neural networks to learn feature crosses inefficiently. In light of the
pros/cons of DCN and existing feature interaction learning approaches, we propose
an improved framework DCN-V2 to make DCN more practical in large-scale industrial
settings. In a comprehensive experimental study with extensive hyper-parameter search
and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art
algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet
remains cost efficient at feature interaction learning, especially when coupled with
a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building
blocks, and has delivered significant offline accuracy and online business metrics
gains across many web-scale learning to rank systems at Google. Our code and tutorial
are open-sourced as part of TensorFlow Recommenders (TFRS)1. },
booktitle = {Proceedings of the Web Conference 2021},
pages = {1785–1797},
numpages = {13},
keywords = {Neural Networks, CTR Prediction, Feature Crossing, Deep Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@misc{WD, title={Wide and Deep Learning: Better Together with TensorFlow}, url={https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html}, journal={Google AI blog}, author={ Cheng, Heng-Tze}, year={2016}, month={Jun}
}

@inproceedings{PROGRESS,
author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
title = {Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3347058},
doi = {10.1145/3298689.3347058},
abstract = {Deep learning techniques have become the method of choice for researchers working
on algorithmic aspects of recommender systems. With the strongly increased interest
in machine learning in general, it has, as a result, become difficult to keep track
of what represents the state-of-the-art at the moment, e.g., for top-n recommendation
tasks. At the same time, several recent publications point out problems in today's
research practice in applied machine learning, e.g., in terms of the reproducibility
of the results or the choice of the baselines when proposing new models.In this work,
we report the results of a systematic analysis of algorithmic proposals for top-n
recommendation tasks. Specifically, we considered 18 algorithms that were presented
at top-level research conferences in the last years. Only 7 of them could be reproduced
with reasonable effort. For these methods, it however turned out that 6 of them can
often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor
or graph-based techniques. The remaining one clearly outperformed the baselines but
did not consistently outperform a well-tuned non-neural linear ranking method. Overall,
our work sheds light on a number of potential problems in today's machine learning
scholarship and calls for improved scientific practices in this area.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {101–109},
numpages = {9},
keywords = {deep learning, reproducibility, evaluation, recommender systems},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{DALMANN,
author = {Dallmann, Alexander and Zoller, Daniel and Hotho, Andreas},
title = {A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3475943},
doi = {10.1145/3460231.3475943},
abstract = {At the present time, sequential item recommendation models are compared by calculating
metrics on a small item subset (target set) to speed up computation. The target set
contains the relevant item and a set of negative items that are sampled from the full
item set. Two well-known strategies to sample negative items are uniform random sampling
and sampling by popularity to better approximate the item frequency distribution in
the dataset. Most recently published papers on sequential item recommendation rely
on sampling by popularity to compare the evaluated models. However, recent work has
already shown that an evaluation with uniform random sampling may not be consistent
with the full ranking, that is, the model ranking obtained by evaluating a metric
using the full item set as target set, which raises the question whether the ranking
obtained by sampling by popularity is equal to the full ranking. In this work, we
re-evaluate current state-of-the-art sequential recommender models from the point
of view, whether these sampling strategies have an impact on the final ranking of
the models. We therefore train four recently proposed sequential recommendation models
on five widely known datasets. For each dataset and model, we employ three evaluation
strategies. First, we compute the full model ranking. Then we evaluate all models
on a target set sampled by the two different sampling strategies, uniform random sampling
and sampling by popularity with the commonly used target set size of 100, compute
the model ranking for each strategy and compare them with each other. Additionally,
we vary the size of the sampled target set. Overall, we find that both sampling strategies
can produce inconsistent rankings compared with the full ranking of the models. Furthermore,
both sampling by popularity and uniform random sampling do not consistently produce
the same ranking when compared over different sample sizes. Our results suggest that
like uniform random sampling, rankings obtained by sampling by popularity do not equal
the full ranking of recommender models and therefore both should be avoided in favor
of the full ranking when establishing state-of-the-art. },
booktitle = {Fifteenth ACM Conference on Recommender Systems},
pages = {505–514},
numpages = {10},
keywords = {Evaluation, Metrics, Sequential Item Recommendation, Sampled Metrics},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@misc{MASKNET,
      title={MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask}, 
      author={Zhiqiang Wang and Qingyun She and Junlin Zhang},
      year={2021},
      eprint={2102.07619},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{DIN,
      title={Deep Interest Network for Click-Through Rate Prediction}, 
      author={Guorui Zhou and Chengru Song and Xiaoqiang Zhu and Ying Fan and Han Zhu and Xiao Ma and Yanghui Yan and Junqi Jin and Han Li and Kun Gai},
      year={2018},
      eprint={1706.06978},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{CLIP,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{METH,
      title={Methodologies for Improving Modern Industrial Recommender Systems},
      author={Shusen Wang},
      year={2023},
      eprint={2308.01204},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{AIRBNB2,
      title={Improving Deep Learning For Airbnb Search}, 
      author={Malay Haldar and Mustafa Abdool and Prashant Ramanathan and Tyler Sax and Lanbo Zhang and Aamir Mansawala and Shulin Yang and Bradley Turnbull and Junshuo Liao},
      year={2020},
      eprint={2002.05515},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{NMT,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{DLRM,
      title={Deep Learning Recommendation Model for Personalization and Recommendation Systems}, 
      author={Maxim Naumov and Dheevatsa Mudigere and Hao-Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
      year={2019},
      eprint={1906.00091},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
